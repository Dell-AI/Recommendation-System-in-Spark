{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "\n",
    "# spark imports\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# data science imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read in data through spark since the data is sored in hadoop and format the columns\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql.functions import *\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Classification\n",
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Regression\n",
    "from pyspark.ml.regression import RandomForestRegressor, RandomForestRegressionModel\n",
    "from pyspark.ml.regression import GBTRegressor, GBTRegressionModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'hdfs:///user/andrew/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf indicates spark dataframe\n",
    "movies = sqlContext.read.parquet(data_path + 'movie_20m_metadata_OHE_subset')\n",
    "users_full = sqlContext.read.parquet(data_path + 'users_metadata_20m') \n",
    "users_full = users_full.na.fill(0)\n",
    "\n",
    "ratings = sqlContext.read.parquet(data_path + 'ratings_20m')\n",
    "ratings = ratings.drop('timestamp')\n",
    "ratings = ratings.withColumn(\"userId\", ratings[\"userId\"].cast(\"int\"))\n",
    "ratings = ratings.withColumn(\"rating\", ratings[\"rating\"] * 2) #Multiply by 2 so that values are whole numbers -> values 1 to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_metadata = ratings.join(movies, ratings.movieId == movies.item_id)\n",
    "# ratings with metadata and users full\n",
    "ratings_muf = ratings_metadata.join(users_full, ratings.userId == users_full.user_id) \\\n",
    "        .drop('userId', 'user_id', 'movieId', 'item_id', 'title', 'imdb_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings with metadata and users full converted to a dataframe of label and features\n",
    "ratings_muf_rdd = ratings_muf.rdd.map(lambda x: (x[0], Vectors.dense(x[1:])))\n",
    "ratings_muf_2 = sqlContext.createDataFrame(ratings_muf_rdd, schema = ['label', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmuf_train, rmuf_test = ratings_muf_2.randomSplit([0.75, 0.25], seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata and Full User Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 118 ms, sys: 49.1 ms, total: 167 ms\n",
      "Wall time: 10min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rfc  = RandomForestClassifier(numTrees=200, maxDepth=10, \n",
    "                              labelCol=\"label\", seed=42, \n",
    "                              maxMemoryInMB= 8192, \n",
    "                              featureSubsetStrategy = 'auto',\n",
    "                              minInstancesPerNode = 20,\n",
    "                              # minInfoGain = ?,\n",
    "                              subsamplingRate = 0.6,\n",
    "                              maxBins = 10)\n",
    "rfc_model = rfc.fit(rmuf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  1.0|[0.0,0.0,0.0,0.0,...|[0.0,1.9943577525...|[0.0,0.0099717887...|       8.0|\n",
      "|  1.0|[0.0,0.0,0.0,88.0...|[0.0,10.307658893...|[0.0,0.0515382944...|       8.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "CPU times: user 3.42 ms, sys: 1.82 ms, total: 5.24 ms\n",
      "Wall time: 33.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rfc_model_preds = rfc_model.transform(rmuf_test)\n",
    "rfc_model_preds.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.264468191464\n",
      "Weighted Precision score: 0.336493997634\n",
      "Weighted Recall score: 0.345180319496\n",
      "Accuracy: 0.345180319496\n",
      "CPU times: user 123 ms, sys: 33.2 ms, total: 156 ms\n",
      "Wall time: 10min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print 'f1 score:', evaluator.evaluate(rfc_model_preds)\n",
    "print 'Weighted Precision score:', evaluator.evaluate(rfc_model_preds, {evaluator.metricName: \"weightedPrecision\"})\n",
    "print 'Weighted Recall score:', evaluator.evaluate(rfc_model_preds, {evaluator.metricName: \"weightedRecall\"})\n",
    "print 'Accuracy:', evaluator.evaluate(rfc_model_preds, {evaluator.metricName: \"accuracy\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 201 ms, sys: 70.6 ms, total: 272 ms\n",
      "Wall time: 30min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rfr  = RandomForestRegressor(numTrees=200, maxDepth=10, \n",
    "                              labelCol=\"label\", seed=42, \n",
    "                              maxMemoryInMB= 8192, \n",
    "                              featureSubsetStrategy = 'auto',\n",
    "                              minInstancesPerNode = 20,\n",
    "                              # minInfoGain = ?,\n",
    "                              subsamplingRate = 0.6)\n",
    "rfr_model = rfr.fit(rmuf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------------+\n",
      "|label|            features|        prediction|\n",
      "+-----+--------------------+------------------+\n",
      "|  1.0|[0.0,0.0,0.0,0.0,...| 6.173916921134639|\n",
      "|  1.0|[0.0,0.0,0.0,88.0...|5.1369872989845575|\n",
      "+-----+--------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfr_model_preds = rfr_model.transform(rmuf_test)\n",
    "rfr_model_preds.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE score: 1.68841502653\n",
      "R-squared: 0.355981716853\n",
      "Mean Absolute Error: 1.29749801383\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\")\n",
    "print 'RMSE score:', evaluator.evaluate(rfr_model_preds)\n",
    "print 'R-squared:', evaluator.evaluate(rfr_model_preds, {evaluator.metricName: \"r2\"})\n",
    "print 'Mean Absolute Error:', evaluator.evaluate(rfr_model_preds, {evaluator.metricName: \"mae\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Tree Regressor\n",
    "##### Depth 30 (Long run time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gbtr  = GBTRegressor(maxDepth=30, labelCol=\"label\", seed=42, subsamplingRate=0.7, stepSize = 0.1, maxMemoryInMB= 8192)\n",
    "gbtr_model = gbtr.fit(rmuf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gbtr_model_preds = gbtr_model.transform(rmuf_test)\n",
    "gbtr_model_preds.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\")\n",
    "print 'RMSE score:', evaluator.evaluate(gbtr_model_preds)\n",
    "print 'R-squared:', evaluator.evaluate(gbtr_model_preds, {evaluator.metricName: \"r2\"})\n",
    "print 'Mean Absolute Error:', evaluator.evaluate(gbtr_model_preds, {evaluator.metricName: \"mae\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Depth 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.27 s, sys: 535 ms, total: 1.81 s\n",
      "Wall time: 11min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gbtr_2  = GBTRegressor(maxDepth=10, labelCol=\"label\", seed=42, stepSize = 0.1, maxMemoryInMB= 2048)\n",
    "gbtr_model_2 = gbtr_2.fit(rmuf_train)\n",
    "# gbtr_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----------------+\n",
      "|label|            features|       prediction|\n",
      "+-----+--------------------+-----------------+\n",
      "|  1.0|[0.0,0.0,0.0,0.0,...|5.205399228048784|\n",
      "|  1.0|[0.0,0.0,0.0,88.0...|4.049581997039118|\n",
      "+-----+--------------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "CPU times: user 1.66 ms, sys: 2.47 ms, total: 4.13 ms\n",
      "Wall time: 31.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gbtr_model_preds_2 = gbtr_model_2.transform(rmuf_test)\n",
    "gbtr_model_preds_2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE score: 1.60541041015\n",
      "R-squared: 0.417746739554\n",
      "Mean Absolute Error: 1.22559769904\n",
      "CPU times: user 58 ms, sys: 9.13 ms, total: 67.1 ms\n",
      "Wall time: 7min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\")\n",
    "print 'RMSE score:', evaluator.evaluate(gbtr_model_preds_2)\n",
    "print 'R-squared:', evaluator.evaluate(gbtr_model_preds_2, {evaluator.metricName: \"r2\"})\n",
    "print 'Mean Absolute Error:', evaluator.evaluate(gbtr_model_preds_2, {evaluator.metricName: \"mae\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load model\n",
    "gbtr_model_2.save(data_path + 'GBTRegD10Model_20m')\n",
    "# sameModel = GBTRegressionModel.load(sc, 'hdfs:///user/andrew/GBTRegD10Model_20m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sameModel = GBTRegressionModel.load(data_path + 'GBTRegD10Model_20m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sameModel_preds_2 = sameModel.transform(rmuf_test)\n",
    "sameModel_preds_2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\")\n",
    "print 'RMSE score:', evaluator.evaluate(sameModel_preds_2)\n",
    "print 'R-squared:', evaluator.evaluate(sameModel_preds_2, {evaluator.metricName: \"r2\"})\n",
    "print 'Mean Absolute Error:', evaluator.evaluate(sameModel_preds_2, {evaluator.metricName: \"mae\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "# gbtr = GBTRegressor(seed = 42, maxMemoryInMB= 2048)\n",
    "# grid = ParamGridBuilder() \\\n",
    "#         .addGrid(gbtr.maxDepth, [10, 20]) \\\n",
    "#         .addGrid(gbtr.subsamplingRate, [0.6, 0.7]) \\\n",
    "#         .addGrid(gbtr.stepSize, [0.01, 0.05, 0.1]) \\\n",
    "#         .build()\n",
    "# evaluator = RegressionEvaluator(predictionCol=\"prediction\", metricName=\"mae\")\n",
    "# cv = CrossValidator(estimator=gbtr, estimatorParamMaps=grid, evaluator=evaluator, seed = 42)\n",
    "# cv_model = cv.fit(rmuf_sdf_train_2)\n",
    "# evaluator.evaluate(cv_model.transform(rmuf_sdf_train_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator.evaluate(cv_model.transform(rmuf_sdf_test_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = [{p.name: v for p, v in m.items()} for m in cv_model.getEstimatorParamMaps()]\n",
    "# [ps.update({cv_model.getEvaluator().getMetricName(): metric}) for ps, metric in zip(params, cv_model.avgMetrics)]\n",
    "# params_df = pd.DataFrame(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 'Best Parameters:'\n",
    "# params_df.iloc[np.where(np.argmin(params_df.metric))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
