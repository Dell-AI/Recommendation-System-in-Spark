{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Collaborative Filtering with both ALS and NCF models for 20M rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we implement ALS and NCF models for Movie Recommendation System for 1M movie ratings. The 20M reviews dataset contains 20 million reviews made by 138,000 users on 27,000 movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "\n",
    "# spark sql imports\n",
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "from pyspark.sql.functions import UserDefinedFunction, explode, desc, rank, col, row_number\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# spark ml imports\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# spark bigdl, analytics zoo imports\n",
    "from zoo.models.recommendation import UserItemFeature\n",
    "from zoo.models.recommendation import NeuralCF\n",
    "from zoo.common.nncontext import init_nncontext\n",
    "from bigdl.nn.criterion import *\n",
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.util.common import *\n",
    "\n",
    "# data science imports\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'hdfs:///user/andrew/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = init_nncontext(\"NCF Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SQLContext for reading in parquet files as Spark dataframes\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Load in the ratings data and format such that it has 3 columns - userId, movieId, rating\n",
    "# The ratings data will be used for modeling and making recommendations\n",
    "ratings = sqlContext.read.parquet(data_path + 'ratings_20m')\n",
    "ratings = ratings.drop('timestamp')\n",
    "ratings = ratings.withColumn(\"userId\", ratings[\"userId\"].cast(\"int\"))\n",
    "ratings = ratings.withColumn(\"rating\", ratings[\"rating\"] * 2) #Multiply by 2 so that values are whole numbers -> values 1 to 10\n",
    "\n",
    "# Load in the movies data and format such that it contains 3 columns - movieId, title, genres\n",
    "# The movies data will be used in the final step to understand what items have been recommended\n",
    "movies = sqlContext.read.parquet(data_path + 'movies_20m')\n",
    "movies = movies.drop('imdbId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|130432|   3003|  10.0|\n",
      "|130432|   3006|  10.0|\n",
      "|130432|   3010|   8.0|\n",
      "|130432|   3052|  10.0|\n",
      "|130432|   3055|  10.0|\n",
      "+------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------+\n",
      "|movieId|               title|       genres|\n",
      "+-------+--------------------+-------------+\n",
      "|  66509| Funny People (2009)| Comedy|Drama|\n",
      "|  66511|Berlin Calling (2...| Comedy|Drama|\n",
      "|  66513|Devil Hides in Do...|  Documentary|\n",
      "|  66517|Against the Dark ...|Action|Horror|\n",
      "|  66537|Letter for the Ki...|    Adventure|\n",
      "+-------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random split results in 16003582 reviews in the training dataset and 3996681 reviews in the validation dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(userId=130432, movieId=3003, rating=10.0),\n",
       " Row(userId=130432, movieId=3055, rating=10.0),\n",
       " Row(userId=130432, movieId=3083, rating=6.0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_train, ratings_val = ratings.randomSplit([0.8, 0.2], seed = 42)\n",
    "print('The random split results in %s reviews in the training dataset and %s reviews in the validation dataset.' \n",
    "      % (ratings_train.count(), ratings_val.count()))\n",
    "ratings_train.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the training and validation datasets into RDDs of Sample. This is the distributed format \n",
    "# used in Analytics Zoo and BigDL to speed up processing time.\n",
    "def build_sample(user_id, item_id, rating):\n",
    "    sample = Sample.from_ndarray(np.array([user_id, item_id]), np.array([rating]))\n",
    "    return UserItemFeature(user_id, item_id, sample)\n",
    "\n",
    "fullPairFeatureRdds = ratings.rdd.map(lambda x: build_sample(x[0], x[1], x[2]))\n",
    "trainPairFeatureRdds = ratings_train.rdd.map(lambda x: build_sample(x[0], x[1], x[2]))\n",
    "valPairFeatureRdds = ratings_val.rdd.map(lambda x: build_sample(x[0], x[1], x[2]))\n",
    "\n",
    "full_rdd = fullPairFeatureRdds.map(lambda pair_feature: pair_feature.sample)\n",
    "train_rdd = trainPairFeatureRdds.map(lambda pair_feature: pair_feature.sample)\n",
    "val_rdd = valPairFeatureRdds.map(lambda pair_feature: pair_feature.sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sample: features: [JTensor: storage: [130432.   3003.], shape: [2], float], labels: [JTensor: storage: [10.], shape: [1], float],\n",
       " Sample: features: [JTensor: storage: [130432.   3055.], shape: [2], float], labels: [JTensor: storage: [10.], shape: [1], float],\n",
       " Sample: features: [JTensor: storage: [130432.   3083.], shape: [2], float], labels: [JTensor: storage: [6.], shape: [1], float]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the first three rows of the training data to better understand what a RDD of Sample looks like.\n",
    "train_rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS and NCF Model Training and Validation on Training data\n",
    "Train ALS and NCF models and compare the Mean Absolte Error (MAE) for each on the validation set. With the parameter settings set below, the ALS model has slightly lower validation error, but also takes far less time to train. However, when comparing the training and validation error for each model, the ALS model is more over fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.03 ms, sys: 2.06 ms, total: 10.1 ms\n",
      "Wall time: 19.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "als = ALS(seed = 42, regParam = 0.1, maxIter = 15, rank = 12,\n",
    "          userCol = \"userId\", itemCol = \"movieId\", ratingCol = \"rating\")\n",
    "evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "als_model = als.fit(ratings_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error (MAE): 1.12183070622\n",
      "Validation Error (MAE): 1.21714696856\n",
      "CPU times: user 7.63 ms, sys: 1.86 ms, total: 9.48 ms\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print 'Training Error (MAE):', evaluator.evaluate(als_model.transform(ratings_train))\n",
    "print 'Validation Error (MAE):', evaluator.evaluate(als_model.transform(ratings_val).fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error (MAE): 1.12183070622\n",
      "Validation Error (MAE): 1.21714696856\n"
     ]
    }
   ],
   "source": [
    "# Save ALS model (trained on all 20M reviews)\n",
    "als_model.write().overwrite().save(path = data_path + 'ALS_Model_test.h5')\n",
    "\n",
    "als_model_test = ALSModel.load(path = data_path + 'ALS_Model_test.h5')\n",
    "\n",
    "print 'Training Error (MAE):', evaluator.evaluate(als_model_test.transform(ratings_train))\n",
    "print 'Validation Error (MAE):', evaluator.evaluate(als_model_test.transform(ratings_val).fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createZooNeuralCF\n",
      "creating: createClassNLLCriterion\n",
      "creating: createMaxEpoch\n",
      "creating: createAdam\n",
      "creating: createDistriOptimizer\n",
      "creating: createEveryEpoch\n",
      "creating: createMAE\n",
      "creating: createClassNLLCriterion\n",
      "creating: createLoss\n",
      "CPU times: user 43.4 ms, sys: 29 ms, total: 72.4 ms\n",
      "Wall time: 5min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 92160\n",
    "max_user_id = ratings.agg({'userId': 'max'}).collect()[0]['max(userId)']\n",
    "max_movie_id = ratings.agg({'movieId': 'max'}).collect()[0]['max(movieId)']\n",
    "\n",
    "ncf = NeuralCF(user_count = max_user_id, item_count = max_movie_id, \n",
    "               class_num = 10, hidden_layers = [20, 10], include_mf = False)\n",
    "\n",
    "optimizer = Optimizer(\n",
    "    model=ncf,\n",
    "    training_rdd=train_rdd,\n",
    "    criterion=ClassNLLCriterion(),\n",
    "    end_trigger=MaxEpoch(10),\n",
    "    batch_size=batch_size, # 16 executors, 16 cores each\n",
    "    optim_method=Adam(learningrate=0.001))\n",
    "\n",
    "optimizer.set_validation(\n",
    "    batch_size=batch_size, # 16 executors, 16 cores each\n",
    "    val_rdd=val_rdd,\n",
    "    trigger=EveryEpoch(),\n",
    "    val_method=[MAE(), Loss(ClassNLLCriterion())]\n",
    ")\n",
    "\n",
    "optimizer.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createMAE\n",
      "creating: createMAE\n",
      "Training Error (MAE): Evaluated result: 1.23713159561, total_num: 44580, method: MAE\n",
      "Validation Error (MAE): Evaluated result: 1.27953612804, total_num: 11238, method: MAE\n",
      "CPU times: user 24.7 ms, sys: 3.41 ms, total: 28.1 ms\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_res = ncf.evaluate(train_rdd, batch_size, [MAE()])\n",
    "val_res = ncf.evaluate(val_rdd, batch_size, [MAE()])\n",
    "\n",
    "print 'Training Error (MAE):', train_res[0]\n",
    "print 'Validation Error (MAE):', val_res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createMAE\n",
      "creating: createMAE\n",
      "Training Error (MAE): Evaluated result: 1.23713171482, total_num: 44580, method: MAE\n",
      "Validation Error (MAE): Evaluated result: 1.27953600883, total_num: 11238, method: MAE\n"
     ]
    }
   ],
   "source": [
    "# Save NCF model (trained on all 20M reviews)\n",
    "ncf.save_model(path = data_path + 'NCF_Model_test.bigdl', \n",
    "               weight_path = data_path + 'NCF_Model_test_weights.bin', \n",
    "               over_write = True)\n",
    "# Load NCF model - compare loaded model results to trained model results\n",
    "ncf_test = NeuralCF.load_model(path = data_path + 'NCF_Model_test.bigdl', \n",
    "                               weight_path = data_path + 'NCF_Model_test_weights.bin')\n",
    "\n",
    "train_res = ncf_test.evaluate(train_rdd, batch_size, [MAE()])\n",
    "val_res = ncf_test.evaluate(val_rdd, batch_size, [MAE()])\n",
    "\n",
    "print 'Training Error (MAE):', train_res[0]\n",
    "print 'Validation Error (MAE):', val_res[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS and NCF Model Training and Validation on the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Error (MAE): 1.13562066895\n",
      "CPU times: user 13.9 ms, sys: 3.29 ms, total: 17.2 ms\n",
      "Wall time: 26.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "als = ALS(seed = 42, regParam = 0.1, maxIter = 15, rank = 12, # coldStartStrategy = 'drop', # drops userIds/movieIds from the validation set or test set so that NaNs are not returned\n",
    "          userCol = \"userId\", itemCol = \"movieId\", ratingCol = \"rating\")\n",
    "evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "als_model = als.fit(ratings)\n",
    "print 'Model Error (MAE):', evaluator.evaluate(als_model.transform(ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ALS model (trained on all 20M reviews)\n",
    "als_model.write().overwrite().save(path = data_path + 'ALS_Model_20m.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createZooNeuralCF\n",
      "creating: createClassNLLCriterion\n",
      "creating: createMaxEpoch\n",
      "creating: createAdam\n",
      "creating: createDistriOptimizer\n",
      "creating: createMAE\n",
      "Model Error (MAE): Evaluated result: 1.24277722836, total_num: 55778, method: MAE\n",
      "CPU times: user 41.8 ms, sys: 30.3 ms, total: 72.1 ms\n",
      "Wall time: 5min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_user_id = ratings.agg({'userId': 'max'}).collect()[0]['max(userId)']\n",
    "max_movie_id = ratings.agg({'movieId': 'max'}).collect()[0]['max(movieId)']\n",
    "ncf = NeuralCF(user_count=max_user_id, item_count=max_movie_id, class_num=10, hidden_layers=[20, 10], include_mf = False)\n",
    "\n",
    "optimizer = Optimizer(\n",
    "    model=ncf,\n",
    "    training_rdd=full_rdd,\n",
    "    criterion=ClassNLLCriterion(),\n",
    "    end_trigger=MaxEpoch(10),\n",
    "    batch_size=batch_size, # 16 executors, 16 cores each\n",
    "    optim_method=Adam(learningrate=0.001))\n",
    "\n",
    "optimizer.optimize()\n",
    "\n",
    "full_res = ncf.evaluate(full_rdd, batch_size, [MAE()])\n",
    "print 'Model Error (MAE):', full_res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save NCF model (trained on all 20M reviews)\n",
    "ncf.save_model(path = data_path + 'NCF_Model_20m.bigdl', \n",
    "               weight_path = data_path + 'NCF_Model_20m_weights.bin', \n",
    "               over_write = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the prediction between ALS and NCF for one specific user. The user id is specified in the final two cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 56s, sys: 50.1 s, total: 4min 46s\n",
      "Wall time: 5min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create a sparse matrix of all combinations of items\n",
    "ratings_df = ratings.toPandas()\n",
    "ratings_matrix = ratings_df.pivot(index='userId',columns='movieId',values='rating').fillna(0)\n",
    "\n",
    "# Melt sparse matrix to dataframe of 3 columns containing userId, movieId, and rating\n",
    "ratings_matrix['userId'] = ratings_matrix.index\n",
    "ratings_df_2 = pd.melt(ratings_matrix, id_vars = ['userId'], value_vars = list(ratings_matrix.columns).remove('userId'))\n",
    "ratings_df_2.columns = ['userId', 'movieId', 'rating']\n",
    "ratings_df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.9 s, sys: 8.83 s, total: 26.7 s\n",
      "Wall time: 26.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Predict for specified user\n",
    "pred_userId = 25643\n",
    "# keep only the userId, movieId pairs that do not have ratings\n",
    "ratings_blanks_df = ratings_df_2.iloc[np.where((ratings_df_2.rating == 0) \n",
    "                                               & (ratings_df_2.userId == pred_userId))]\n",
    "\n",
    "# Convert to spark dataframe\n",
    "ratings_blanks = sqlContext.createDataFrame(ratings_blanks_df)\n",
    "# Create RDD of Sample from the spark dataframe\n",
    "blankPairFeatureRdds = ratings_blanks.rdd.map(lambda x: build_sample(x[0], x[1], x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.4 ms, sys: 15.1 ms, total: 69.5 ms\n",
      "Wall time: 5.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "als_pair_preds = als_model.transform(ratings_blanks)\n",
    "ncf_pair_preds = ncf.recommend_for_user(blankPairFeatureRdds, 10).toDF()\n",
    "\n",
    "als_preds = als_pair_preds.select('userId', 'movieId', 'prediction').toDF('userId', 'movieId', 'als_pred')\n",
    "ncf_preds_topN = ncf_pair_preds.select('user_id', 'item_id', 'prediction').toDF('userId', 'movieId', 'ncf_pred')\n",
    "del als_pair_preds, ncf_pair_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.9 ms, sys: 12.7 ms, total: 65.7 ms\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "window = Window.partitionBy(als_preds['userId']).orderBy(als_preds['als_pred'].desc())\n",
    "als_preds_topN = als_preds.select(col('*'), row_number().over(window).alias('row_number')).where(col('row_number') <= 10)\n",
    "\n",
    "als_preds_topN_labeled = als_preds_topN.join(movies, how = 'left', on = 'movieId')\n",
    "ncf_preds_topN_labeled = ncf_preds_topN.join(movies, how = 'left', on = 'movieId')\n",
    "\n",
    "als_final = als_preds_topN_labeled.select('userId', 'movieId', 'als_pred', 'title').sort(col(\"userId\")).toPandas()\n",
    "ncf_final = ncf_preds_topN_labeled.select('userId', 'movieId', 'ncf_pred', 'title').sort(col(\"userId\")).toPandas()\n",
    "del window, als_preds, als_preds_topN, ncf_preds_topN, als_preds_topN_labeled, ncf_preds_topN_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>als_pred</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25643</td>\n",
       "      <td>77947</td>\n",
       "      <td>10.808342</td>\n",
       "      <td>Harishchandrachi Factory (2009)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25643</td>\n",
       "      <td>94222</td>\n",
       "      <td>10.600865</td>\n",
       "      <td>Don't Eat the Pictures: Sesame Street at the M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25643</td>\n",
       "      <td>87164</td>\n",
       "      <td>10.402793</td>\n",
       "      <td>Henri-Georges Clouzot's Inferno (L'enfer d'Hen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25643</td>\n",
       "      <td>113218</td>\n",
       "      <td>10.314117</td>\n",
       "      <td>Space Milkshake (2012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25643</td>\n",
       "      <td>109786</td>\n",
       "      <td>10.275653</td>\n",
       "      <td>Carmina and Amen (Carmina y amÃ©n) (2014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25643</td>\n",
       "      <td>82836</td>\n",
       "      <td>10.231801</td>\n",
       "      <td>Life of Reilly, The (2006)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25643</td>\n",
       "      <td>110669</td>\n",
       "      <td>10.168848</td>\n",
       "      <td>Honest Liar, An (2014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25643</td>\n",
       "      <td>68273</td>\n",
       "      <td>10.152931</td>\n",
       "      <td>Amazing Journey: The Story of The Who (2007)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25643</td>\n",
       "      <td>116951</td>\n",
       "      <td>10.125403</td>\n",
       "      <td>Bo Burnham: what. (2013)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25643</td>\n",
       "      <td>43267</td>\n",
       "      <td>9.998287</td>\n",
       "      <td>On Probation (Tiempo de Valientes) (2005)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId   als_pred  \\\n",
       "0   25643    77947  10.808342   \n",
       "1   25643    94222  10.600865   \n",
       "2   25643    87164  10.402793   \n",
       "3   25643   113218  10.314117   \n",
       "4   25643   109786  10.275653   \n",
       "5   25643    82836  10.231801   \n",
       "6   25643   110669  10.168848   \n",
       "7   25643    68273  10.152931   \n",
       "8   25643   116951  10.125403   \n",
       "9   25643    43267   9.998287   \n",
       "\n",
       "                                               title  \n",
       "0                    Harishchandrachi Factory (2009)  \n",
       "1  Don't Eat the Pictures: Sesame Street at the M...  \n",
       "2  Henri-Georges Clouzot's Inferno (L'enfer d'Hen...  \n",
       "3                             Space Milkshake (2012)  \n",
       "4          Carmina and Amen (Carmina y amÃ©n) (2014)  \n",
       "5                         Life of Reilly, The (2006)  \n",
       "6                             Honest Liar, An (2014)  \n",
       "7       Amazing Journey: The Story of The Who (2007)  \n",
       "8                           Bo Burnham: what. (2013)  \n",
       "9          On Probation (Tiempo de Valientes) (2005)  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>ncf_pred</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25643</td>\n",
       "      <td>105250</td>\n",
       "      <td>9</td>\n",
       "      <td>Century of the Self, The (2002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25643</td>\n",
       "      <td>101850</td>\n",
       "      <td>9</td>\n",
       "      <td>Death on the Staircase (SoupÃ§ons) (2004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25643</td>\n",
       "      <td>86237</td>\n",
       "      <td>9</td>\n",
       "      <td>Connections (1978)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25643</td>\n",
       "      <td>77658</td>\n",
       "      <td>9</td>\n",
       "      <td>Cosmos (1980)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25643</td>\n",
       "      <td>318</td>\n",
       "      <td>9</td>\n",
       "      <td>Shawshank Redemption, The (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25643</td>\n",
       "      <td>93040</td>\n",
       "      <td>9</td>\n",
       "      <td>Civil War, The (1990)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25643</td>\n",
       "      <td>7502</td>\n",
       "      <td>9</td>\n",
       "      <td>Band of Brothers (2001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25643</td>\n",
       "      <td>26587</td>\n",
       "      <td>9</td>\n",
       "      <td>Decalogue, The (Dekalog) (1989)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25643</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>Seven Samurai (Shichinin no samurai) (1954)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25643</td>\n",
       "      <td>114635</td>\n",
       "      <td>9</td>\n",
       "      <td>Look of Silence, The (2014)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  ncf_pred                                        title\n",
       "0   25643   105250         9              Century of the Self, The (2002)\n",
       "1   25643   101850         9    Death on the Staircase (SoupÃ§ons) (2004)\n",
       "2   25643    86237         9                           Connections (1978)\n",
       "3   25643    77658         9                                Cosmos (1980)\n",
       "4   25643      318         9             Shawshank Redemption, The (1994)\n",
       "5   25643    93040         9                        Civil War, The (1990)\n",
       "6   25643     7502         9                      Band of Brothers (2001)\n",
       "7   25643    26587         9              Decalogue, The (Dekalog) (1989)\n",
       "8   25643     2019         9  Seven Samurai (Shichinin no samurai) (1954)\n",
       "9   25643   114635         9                  Look of Silence, The (2014)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncf_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
