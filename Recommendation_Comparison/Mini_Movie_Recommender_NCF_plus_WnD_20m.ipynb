{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions are from or based on work done by Kevin Liao in the below notebook\n",
    "# https://github.com/KevinLiao159/MyDataSciencePortfolio/blob/master/movie_recommender/movie_recommendation_using_ALS.ipynb\n",
    "\n",
    "# Intialization\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import datetime as df\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "\n",
    "# spark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import UserDefinedFunction, explode, desc\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from zoo.models.recommendation import *\n",
    "from zoo.models.recommendation.utils import *\n",
    "from zoo.common.nncontext import init_nncontext\n",
    "\n",
    "from bigdl.dataset.transformer import *\n",
    "from bigdl.dataset.base import *\n",
    "from bigdl.nn.criterion import *\n",
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.util.common import *\n",
    "from bigdl.nn.layer import *\n",
    "\n",
    "# data science imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "init_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'hdfs:///user/andrew/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.62 s, sys: 151 ms, total: 2.77 s\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Read in data through spark since the data is sored in hadoop and format the columns\n",
    "# Convert to pandas dataframes for easier and faster manipulation\n",
    "sqlContext = SQLContext(sc)\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "movies = sqlContext.read.parquet(data_path + 'movie_20m_metadata_OHE_subset')\n",
    "movies_df = movies.toPandas()\n",
    "movies_df = movies_df.set_index(movies_df.item_id) # set index so no sorting errors occur\n",
    "\n",
    "movies_gp = movies.drop('title', 'imdb_id', 'imdb_rating', 'imdb_votes', 'metascore', 'runtime', 'year')\n",
    "\n",
    "users_full_sdf = sqlContext.read.parquet(data_path + 'users_metadata_20m') \n",
    "users_full_sdf = users_full_sdf.na.fill(0)\n",
    "\n",
    "ratings_initial = sqlContext.read.parquet(data_path + 'ratings_20m')\n",
    "ratings_initial = ratings_initial.drop('timestamp')\n",
    "ratings_initial = ratings_initial.withColumn(\"userId\", ratings_initial[\"userId\"].cast(\"int\"))\n",
    "#Multiply ratings by 2 so that values are whole numbers -> values 1 to 10\n",
    "ratings_initial = ratings_initial.withColumn(\"label\", ratings_initial[\"rating\"] * 2) \n",
    "ratings = ratings_initial.select(\"userId\", \"movieId\", \"label\").toDF(\"userId\", \"itemId\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input function - takes user input data, strpis it down, and calls other functions on that data\n",
    "# Takes in user age, gender, occupation (of 20 options - may drop this), list of favorite movies\n",
    "# All movies in the list of favorite movies will be rated 5 stars\n",
    "def new_user_input(fav_movies, all_ratings, movies, spark_context, \n",
    "                   sqlContext = None, num_recs = 10, movies_gp = None, movies_df = None):\n",
    "    # collect favorite movie ids\n",
    "    print 'Collecting favorite movie IDs'\n",
    "    movieIds = get_movieId(movies_df, fav_movies)\n",
    "    # print 'Favorite movies in the available set'\n",
    "    # print movies_df[['item_id', 'title', 'year']].loc[movieIds]\n",
    "    \n",
    "    print 'Adding ratings to full set'\n",
    "    # add new user movie ratings to all ratings dataframe\n",
    "    # all_ratings_updated, user_ratings, user_ratings_binary = add_new_user_to_data(all_ratings, movieIds, spark_context)\n",
    "    all_ratings_updated, new_user_ratings = add_new_user_to_data(all_ratings, movieIds, spark_context)\n",
    "    del all_ratings\n",
    "    \n",
    "    print 'Creating prediction set'\n",
    "    # get all unrated movies for user (unnecessary in Spark 2.2+, instead use the recommendForAllUsers(num_to_rec) method)\n",
    "    all_user_unrated = get_inference_data(all_ratings_updated, movieIds)\n",
    "    \n",
    "    print 'Formatting training and prediction dataframes for NCF'\n",
    "    # Fornat ratings data into RDD Samples (the format needed for Analytics Zoo models)\n",
    "    trainPairFeatureRdds = all_ratings_updated.rdd.map(lambda x: build_sample(x[0], x[1], x[2]))\n",
    "    predPairFeatureRdds = all_user_unrated.rdd.map(lambda x: build_sample(x[0], x[1], x[2]))\n",
    "\n",
    "    train_rdd = trainPairFeatureRdds.map(lambda pair_feature: pair_feature.sample)\n",
    "    pred_rdd = predPairFeatureRdds.map(lambda pair_feature: pair_feature.sample)\n",
    "    \n",
    "    print 'Training NCF Model'\n",
    "    # train NCF model, then predict movie ratings    \n",
    "    batch_size = 46080 #16 executors, 16 cores each\n",
    "    max_user_id = all_ratings_updated.agg({'userId': 'max'}).collect()[0]['max(userId)']\n",
    "    max_movie_id = all_ratings_updated.agg({'itemId': 'max'}).collect()[0]['max(itemId)']\n",
    "\n",
    "    ncf = NeuralCF(user_count = max_user_id, item_count = max_movie_id, \n",
    "                   class_num = 10, hidden_layers = [20, 10], include_mf = False)\n",
    "\n",
    "    optimizer = Optimizer(\n",
    "        model=ncf,\n",
    "        training_rdd=train_rdd,\n",
    "        criterion=ClassNLLCriterion(),\n",
    "        end_trigger=MaxEpoch(10),\n",
    "        batch_size=batch_size, # 16 executors, 16 cores each\n",
    "        optim_method=Adam(learningrate=0.001))\n",
    "\n",
    "    optimizer.optimize()\n",
    "    # del all_ratings_updated\n",
    "    \n",
    "    print 'Making Predictions'\n",
    "    # keep top 30 predictions\n",
    "    full_predictions_sorted = ncf.recommend_for_user(predPairFeatureRdds, 3*num_recs).toDF().sort(desc('prediction'))\n",
    "    ncf_top_n_predictions = full_predictions_sorted.take(num_recs)\n",
    "    # extract movie ids\n",
    "    ncf_top_n_ids = [r[1] for r in ncf_top_n_predictions]\n",
    "    \n",
    "    ncf_movie_recs = movies.filter(movies.item_id.isin(ncf_top_n_ids)).select('title', 'year')\n",
    "    print ''\n",
    "    print 'NCF Recommendations'\n",
    "    print ncf_movie_recs.toPandas()\n",
    "    \n",
    "    \n",
    "    # import WnD model input data format\n",
    "    # Create user_id x item_id matrix need to get data in the form of user_id, item_id, label, then pivot\n",
    "    # filter movies_gp dataframe by the movieIds. pivot new_user_ratings into a vector, \n",
    "    # then multiply by the filtered movies_gp dataframe; divide by binarized user ratings; \n",
    "    # this should now be a vector of user preferences. \n",
    "    # join a OHE age, gender, and possibly occupation, to the user preferences\n",
    "    user_summary_sdf = get_user_preferences(user_ratings = new_user_ratings, movieIds = movieIds, \n",
    "                                            movies_gp = movies_gp, sqlContext = sqlContext)\n",
    "\n",
    "    ncf_top_3xn_ids = full_predictions_sorted.select('item_id')\n",
    "    all_user_unrated_top_3xn = all_user_unrated.join(ncf_top_3xn_ids, all_user_unrated.itemId == ncf_top_3xn_ids.item_id, \n",
    "                                                     'inner').drop(ncf_top_3xn_ids.item_id)\n",
    "    top_3xn_movies_metadata = movies.join(ncf_top_3xn_ids, movies.item_id == ncf_top_3xn_ids.item_id, \n",
    "                                          'inner').drop(ncf_top_3xn_ids.item_id)\n",
    "        \n",
    "    # lastly, replicate the user pref rows for each rated movieId, then join with the filtered movies dataframe\n",
    "    # (MAKE SURE ALL COLUMNS ARE ORDERED AND NAMED CORRECTLY)\n",
    "    unrated_with_movie_metadata = all_user_unrated_top_3xn \\\n",
    "                                    .join(top_3xn_movies_metadata, \n",
    "                                          all_user_unrated_top_3xn.itemId == top_3xn_movies_metadata.item_id, \n",
    "                                          how = 'left') \\\n",
    "                                    .drop(top_3xn_movies_metadata.item_id)\n",
    "    unrated_with_full_metadata = unrated_with_movie_metadata \\\n",
    "                                    .join(user_summary_sdf, on = 'userId', how = 'left')\n",
    "    # Create lists of columns sets\n",
    "    identifier_fields = ['userId', 'itemId', 'label', 'title', 'imdb_id']\n",
    "    continuous_base_fields = ['imdb_rating', 'imdb_votes', 'metascore', 'runtime', 'year']\n",
    "    all_base_fields = identifier_fields + continuous_base_fields\n",
    "\n",
    "    user_avgs = [col_name for col_name in unrated_with_full_metadata.columns if col_name[-11:] == '_avg_rating']\n",
    "    movie_metadata = [col_name for col_name in unrated_with_full_metadata.columns \n",
    "                      if (col_name[-11:] != '_avg_rating' and col_name not in all_base_fields)]\n",
    "\n",
    "    user_avgs_genres = [genre for genre in user_avgs if 'genre' in genre]\n",
    "    user_avgs_ml_genres = [genre for genre in user_avgs if genre[:8] == 'ml_genre']\n",
    "    user_avgs_imdb_genres = [genre for genre in user_avgs if genre[:10] == 'imdb_genre']\n",
    "    user_avgs_directors = [director for director in user_avgs if director[0:9] == 'director_']\n",
    "    user_avgs_actors = [actor for actor in user_avgs if actor[0:6] == 'actor_']\n",
    "\n",
    "    movie_genres = [genre for genre in movie_metadata if 'genre' in genre]\n",
    "    movie_ml_genres = [genre for genre in movie_metadata if genre[:8] == 'ml_genre']\n",
    "    movie_imdb_genres = [genre for genre in movie_metadata if genre[:10] == 'imdb_genre']\n",
    "    movie_directors = [director for director in movie_metadata if director[0:9] == 'director_']\n",
    "    movie_actors = [actor for actor in movie_metadata if actor[0:6] == 'actor_']\n",
    "    \n",
    "    # Determine embedding dimmensions\n",
    "    max_user_id = unrated_with_full_metadata.agg({\"userId\": \"max\"}).collect()[0][0]\n",
    "    max_movie_id = unrated_with_full_metadata.agg({\"itemId\": \"max\"}).collect()[0][0]\n",
    "    # num_rating_labels = unrated_with_full_metadata.select('label').distinct().count()\n",
    "    \n",
    "    # Create column_info for feature formatting\n",
    "    bucket_size = 100\n",
    "    wide_base_cols = indicator_base_fields + movie_genres + user_avgs_genres\n",
    "    wide_base_dims = [3 for i in (indicator_base_fields + movie_genres)] + [6 for i in user_avgs_genres]\n",
    "    indicator_cols = indicator_base_fields + movie_genres\n",
    "    indicator_dims = [3 for i in indicator_cols]\n",
    "    continuous_cols = continuous_base_fields + user_avgs_genres\n",
    "    column_info = ColumnFeatureInfo(\n",
    "                wide_base_cols = wide_base_cols,\n",
    "                wide_base_dims = wide_base_dims,\n",
    "                # wide_cross_cols = [\"age-gender\"],\n",
    "                # wide_cross_dims = [bucket_size],\n",
    "                indicator_cols = indicator_cols,\n",
    "                indicator_dims = indicator_dims,\n",
    "                embed_cols = [\"userId\", \"itemId\"],\n",
    "                embed_in_dims = [max_user_id, max_movie_id],\n",
    "                embed_out_dims = [500, 500],\n",
    "                continuous_cols = continuous_cols)\n",
    "    \n",
    "    # format coumns to feature\n",
    "    wnd_pred_rdd = unrated_with_full_metadata.rdd.map(lambda row: to_user_item_feature(row, column_info))\n",
    "    \n",
    "    # import the WideAndDeep model\n",
    "    WnDModel = WideAndDeep.load_model(path = data_path + 'WnD_Model_20m.bigdl', \n",
    "                                      weight_path = data_path + 'WnD_Model_20m_weights.h5')\n",
    "    \n",
    "    # recommend items for the new user\n",
    "    wnd_user_recs = WnDModel.recommend_for_user(wnd_pred_rdd, num_recs)\n",
    "    # extract the item_ids for the recommended items\n",
    "    user_recs = [user_rec.item_id for user_rec in wnd_user_recs.take(num_recs)]\n",
    "\n",
    "    # filter the movies sdf for only the recommended items\n",
    "    wnd_movie_recs = movies.filter(col('item_id').isin(user_recs)).select('title', 'year')\n",
    "    print ' '\n",
    "    print 'Wind&Deep Recommendations'\n",
    "    print wnd_movie_recs.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movieId(movies_df, fav_movie_list):\n",
    "    \"\"\"\n",
    "    return all movieId(s) of user's favorite movies\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_movies: spark Dataframe, movies data\n",
    "    \n",
    "    fav_movie_list: list, user's list of favorite movies\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    movieId_list: list of movieId(s)\n",
    "    \"\"\"\n",
    "    movieId_list = []\n",
    "    for movie in fav_movie_list:\n",
    "        if movie[0:4] == 'The ':\n",
    "            movie = movie[4:]\n",
    "        elif movie[0:3] == 'An ':\n",
    "            movie = movie[3:]\n",
    "        elif movie[0:3] == 'La ':\n",
    "            movie = movie[3:]\n",
    "        elif movie[0:2] == 'A ':\n",
    "            movie = movie[3:]\n",
    "\n",
    "        if movie[-6:-5] == '(':\n",
    "            year = int(movie[-5:-1])\n",
    "            movie = movie[0:-7]\n",
    "            movieIds = movies_df.item_id[(movies_df.title.str.contains(movie)) & (movies_df.year == year)]\n",
    "            movieId_list.extend(movieIds)\n",
    "        elif len(movie.split(' ')) == 1:\n",
    "            movieIds = movies_df.item_id[movies_df.title == movie]\n",
    "            movieId_list.extend(movieIds)\n",
    "        else:\n",
    "            movieIds = movies_df.item_id[movies_df.title.str.contains(movie)]\n",
    "            movieId_list.extend(movieIds)\n",
    "    return movieId_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_user_to_data(train_data, movieIds, spark_context):\n",
    "    \"\"\"\n",
    "    add new rows with new user, user's movie and ratings to\n",
    "    existing train data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data: Spark DataFrame, ratings data\n",
    "    \n",
    "    movieIds: spark DataFrame, single column of movieId(s)\n",
    "\n",
    "    spark_context: Spark Context object\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    new train data with the new user's rows\n",
    "    \"\"\"\n",
    "    # get new user id\n",
    "    new_id = train_data.agg({\"userId\": \"max\"}).collect()[0][0] + 1\n",
    "    # get max rating\n",
    "    max_rating = train_data.agg({\"label\": \"max\"}).collect()[0][0]\n",
    "    # create new user sdf for max rating\n",
    "    user_rows_max = [(new_id, movieId, max_rating) for movieId in movieIds]\n",
    "    new_sdf_max = spark_context.parallelize(user_rows_max).toDF(['userId', 'itemId', 'label'])\n",
    "    # return new train data\n",
    "    return train_data.union(new_sdf_max), new_sdf_max # , new_sdf_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_data(train_data, movieIds):\n",
    "    \"\"\"\n",
    "    return a rdd with the userid and all movies (except ones in movieId_list)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data: spark RDD, ratings data\n",
    "\n",
    "    df_movies: spark Dataframe, movies data\n",
    "    \n",
    "    movieId_list: list, list of movieId(s)\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    inference data: Spark RDD\n",
    "    \"\"\"\n",
    "    # get new user id\n",
    "    new_id = train_data.agg({\"userId\": \"max\"}).collect()[0][0]\n",
    "    \n",
    "    distinct_unrated_items = ratings.select('itemId').distinct().filter(~col('itemId').isin(movieIds))\n",
    "\n",
    "    user_unrated = distinct_unrated_items.withColumn('userId', lit(new_id)).select('userId', 'itemId')\n",
    "    user_unrated = user_unrated.withColumn('label', lit(0))\n",
    "    return user_unrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sample(user_id, item_id, rating):\n",
    "    sample = Sample.from_ndarray(np.array([user_id, item_id]), np.array([rating]))\n",
    "    return UserItemFeature(user_id, item_id, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_preferences(user_ratings, movieIds, movies_gp, sqlContext):        \n",
    "    #new_user_ratings\n",
    "    # pivoted_user_ratings = user_ratings.groupBy('user_id').pivot('item_id').agg(avg('label'))\n",
    "    # pivoted_new_user_ratings_binary = user_ratings_binary.groupBy('user_id').pivot('item_id').agg(avg('label')).drop('user_id')\n",
    "    pivoted_user_ratings_df = user_ratings.toPandas() \\\n",
    "                                            .pivot(index='userId', \n",
    "                                                   columns='itemId',\n",
    "                                                   values='label') \\\n",
    "                                            .fillna(0)\n",
    "    pivoted_user_ratings_df_binary = pivoted_user_ratings_df / pivoted_user_ratings_df\n",
    "    \n",
    "    movies_gp_filtered = movies_gp.filter(col('item_id').isin(movieIds))\n",
    "    movies_gp_filtered_df = movies_gp_filtered.toPandas()\n",
    "    # movies_gp_filtered_df.item_id = movies_gp_filtered_df.item_id.astype(str) only necessary when pivot was done on spark df\n",
    "    movies_gp_filtered_df = movies_gp_filtered_df.set_index('item_id')\n",
    "    \n",
    "    user_summary_total = pivoted_user_ratings_df.dot(movies_gp_filtered_df)\n",
    "    user_summary_count = pivoted_user_ratings_df_binary.dot(movies_gp_filtered_df)\n",
    "    user_summary_avg = (user_summary_total / user_summary_count).fillna(0)\n",
    "    user_summary_avg = user_summary_avg.add_suffix('_avg_rating').reset_index()\n",
    "    \n",
    "    sorted_columns = list(user_summary_avg.columns.sort_values())\n",
    "    user_summary_sdf = sqlContext.createDataFrame(user_summary_avg[sorted_columns])\n",
    "    return user_summary_sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by Step Walkthrough of Main Function (to show runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting favorite movie IDs\n",
      "Favorite movies in the available set\n",
      "         item_id                                              title  year\n",
      "item_id                                                                  \n",
      "89753      89753                   Tinker Tailor Soldier Spy (2011)  2011\n",
      "318          318                   Shawshank Redemption, The (1994)  1994\n",
      "2116        2116                      Lord of the Rings, The (1978)  1978\n",
      "7153        7153  Lord of the Rings: The Return of the King, The...  2003\n",
      "4993        4993  Lord of the Rings: The Fellowship of the Ring,...  2001\n",
      "5952        5952      Lord of the Rings: The Two Towers, The (2002)  2002\n",
      "CPU times: user 44.7 ms, sys: 4.33 ms, total: 49 ms\n",
      "Wall time: 48 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fav_movies = ['Tinker Tailor Soldier Spy', 'Shawshank Redemption', 'Lord of the Rings']\n",
    "# collect favorite movie ids\n",
    "print 'Collecting favorite movie IDs'\n",
    "movieIds = get_movieId(movies_df, fav_movies)\n",
    "if movies_df is not None:\n",
    "    print 'Favorite movies in the available set'\n",
    "    print movies_df[['item_id', 'title', 'year']].loc[movieIds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding ratings to full set\n",
      "CPU times: user 23.3 ms, sys: 4.66 ms, total: 28 ms\n",
      "Wall time: 9.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print 'Adding ratings to full set'\n",
    "# add new user movie ratings to all ratings dataframe\n",
    "# all_ratings_updated, user_ratings, user_ratings_binary = add_new_user_to_data(all_ratings, movieIds, spark_context)\n",
    "all_ratings_updated, user_ratings = add_new_user_to_data(ratings, movieIds, sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating prediction set\n",
      "CPU times: user 15.9 ms, sys: 900 µs, total: 16.8 ms\n",
      "Wall time: 3.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print 'Creating prediction set'\n",
    "# get all unrated movies for user (unnecessary in Spark 2.2+, instead use the recommendForAllUsers(num_to_rec) method)\n",
    "all_user_unrated = get_inference_data(all_ratings_updated, movieIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting training and prediction dataframes for NCF\n",
      "CPU times: user 1.88 ms, sys: 0 ns, total: 1.88 ms\n",
      "Wall time: 140 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print 'Formatting training and prediction dataframes for NCF'\n",
    "# Fornat ratings data into RDD Samples (the format needed for Analytics Zoo models)\n",
    "trainPairFeatureRdds = all_ratings_updated.rdd.map(lambda x: build_sample(x[0], x[1], x[2]))\n",
    "predPairFeatureRdds = all_user_unrated.rdd.map(lambda x: build_sample(x[0], x[1], x[2]))\n",
    "\n",
    "train_rdd = trainPairFeatureRdds.map(lambda pair_feature: pair_feature.sample)\n",
    "# pred_rdd = predPairFeatureRdds.map(lambda pair_feature: pair_feature.sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NCF Model\n",
      "creating: createZooNeuralCF\n",
      "creating: createClassNLLCriterion\n",
      "creating: createMaxEpoch\n",
      "creating: createAdam\n",
      "creating: createDistriOptimizer\n",
      "CPU times: user 62.3 ms, sys: 15.6 ms, total: 77.9 ms\n",
      "Wall time: 8min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print 'Training NCF Model'\n",
    "# train NCF model, then predict movie ratings    \n",
    "batch_size = 46080 #16 executors, 16 cores each\n",
    "max_user_id = all_ratings_updated.agg({'userId': 'max'}).collect()[0]['max(userId)']\n",
    "max_item_id = all_ratings_updated.agg({'itemId': 'max'}).collect()[0]['max(itemId)']\n",
    "\n",
    "ncf = NeuralCF(user_count = max_user_id, item_count = max_item_id, \n",
    "               class_num = 10, hidden_layers = [20, 10], include_mf = False)\n",
    "\n",
    "optimizer = Optimizer(\n",
    "    model=ncf,\n",
    "    training_rdd=train_rdd,\n",
    "    criterion=ClassNLLCriterion(),\n",
    "    end_trigger=MaxEpoch(10),\n",
    "    batch_size=batch_size, # 16 executors, 16 cores each\n",
    "    optim_method=Adam(learningrate=0.001))\n",
    "\n",
    "optimizer.optimize()\n",
    "# del all_ratings_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making Predictions\n",
      "\n",
      "NCF Recommendations\n",
      "                                        title  year\n",
      "0                         Love You You (2011)  2011\n",
      "1                  Usual Suspects, The (1995)  1995\n",
      "2   First Name: Carmen (PrAnom Carmen) (1983)  1983\n",
      "3                 How to Die in Oregon (2011)  2011\n",
      "4             History Is Made at Night (1937)  1937\n",
      "5                 Ween Live in Chicago (2004)  2004\n",
      "6    Death on the Staircase (SoupAons) (2004)     0\n",
      "7                Car Bonus (Autobonus) (2001)  2001\n",
      "8                     India's Daughter (2015)  2015\n",
      "9          Witness for the Prosecution (1957)  1957\n",
      "10                Murder on Flight 502 (1975)  1975\n",
      "11                    Dark Knight, The (2008)  2008\n",
      "12               The Salt of the Earth (2014)  2014\n",
      "13                   Symbol (Shinboru) (2009)  2009\n",
      "14                    Schindler's List (1993)  1993\n",
      "CPU times: user 74.5 ms, sys: 10.6 ms, total: 85.2 ms\n",
      "Wall time: 26.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print 'Making Predictions'\n",
    "# keep top 15 predictions\n",
    "num_recs = 15\n",
    "full_predictions_sorted = ncf.recommend_for_user(predPairFeatureRdds, 3*num_recs).toDF().sort(desc('prediction'))\n",
    "ncf_top_n_predictions = full_predictions_sorted.take(num_recs)\n",
    "# extract movie ids\n",
    "ncf_top_n_ids = [r[1] for r in ncf_top_n_predictions]\n",
    "\n",
    "ncf_movie_recs = movies.filter(movies.item_id.isin(ncf_top_n_ids)).select('title', 'year')\n",
    "print ''\n",
    "print 'NCF Recommendations'\n",
    "print ncf_movie_recs.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 104 ms, sys: 30.2 ms, total: 134 ms\n",
      "Wall time: 4.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# import WnD model input data format\n",
    "# Create user_id x item_id matrix need to get data in the form of user_id, item_id, label, then pivot\n",
    "# filter movies_gp dataframe by the movieIds. pivot new_user_ratings into a vector, \n",
    "# then multiply by the filtered movies_gp dataframe; divide by binarized user ratings; \n",
    "# this should now be a vector of user preferences. \n",
    "# join a OHE age, gender, and possibly occupation, to the user preferences\n",
    "user_summary_sdf = get_user_preferences(user_ratings = user_ratings, movieIds = movieIds, \n",
    "                                        movies_gp = movies_gp, sqlContext = sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.81 ms, sys: 1.37 ms, total: 4.18 ms\n",
      "Wall time: 140 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ncf_top_3xn_ids = full_predictions_sorted.select('item_id')\n",
    "all_user_unrated_top_3xn = all_user_unrated.join(ncf_top_3xn_ids, all_user_unrated.itemId == ncf_top_3xn_ids.item_id, \n",
    "                                                 'inner').drop(ncf_top_3xn_ids.item_id)\n",
    "top_3xn_movies_metadata = movies.join(ncf_top_3xn_ids, movies.item_id == ncf_top_3xn_ids.item_id, \n",
    "                                      'inner').drop(ncf_top_3xn_ids.item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.1 ms, sys: 482 µs, total: 7.58 ms\n",
      "Wall time: 550 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# lastly, replicate the user pref rows for each rated movieId, then join with the filtered movies dataframe\n",
    "# (MAKE SURE ALL COLUMNS ARE ORDERED AND NAMED CORRECTLY)\n",
    "unrated_with_movie_metadata = all_user_unrated_top_3xn \\\n",
    "                                .join(top_3xn_movies_metadata, \n",
    "                                      all_user_unrated_top_3xn.itemId == top_3xn_movies_metadata.item_id, \n",
    "                                      how = 'left') \\\n",
    "                                .drop(top_3xn_movies_metadata.item_id)\n",
    "unrated_with_full_metadata = unrated_with_movie_metadata \\\n",
    "                                .join(user_summary_sdf, on = 'userId', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_fields = ['userId', 'itemId', 'label', 'title', 'imdb_id']\n",
    "continuous_base_fields = ['imdb_rating', 'imdb_votes', 'metascore', 'runtime', 'year']\n",
    "all_base_fields = identifier_fields + continuous_base_fields\n",
    "\n",
    "user_avgs = [col_name for col_name in unrated_with_full_metadata.columns if col_name[-11:] == '_avg_rating']\n",
    "movie_metadata = [col_name for col_name in unrated_with_full_metadata.columns \n",
    "                  if (col_name[-11:] != '_avg_rating' and col_name not in all_base_fields)]\n",
    "\n",
    "user_avgs_genres = [genre for genre in user_avgs if 'genre' in genre]\n",
    "user_avgs_ml_genres = [genre for genre in user_avgs if genre[:8] == 'ml_genre']\n",
    "user_avgs_imdb_genres = [genre for genre in user_avgs if genre[:10] == 'imdb_genre']\n",
    "user_avgs_directors = [director for director in user_avgs if director[0:9] == 'director_']\n",
    "user_avgs_actors = [actor for actor in user_avgs if actor[0:6] == 'actor_']\n",
    "\n",
    "movie_genres = [genre for genre in movie_metadata if 'genre' in genre]\n",
    "movie_ml_genres = [genre for genre in movie_metadata if genre[:8] == 'ml_genre']\n",
    "movie_imdb_genres = [genre for genre in movie_metadata if genre[:10] == 'imdb_genre']\n",
    "movie_directors = [director for director in movie_metadata if director[0:9] == 'director_']\n",
    "movie_actors = [actor for actor in movie_metadata if actor[0:6] == 'actor_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_user_id = unrated_with_full_metadata.agg({\"userId\": \"max\"}).collect()[0][0]\n",
    "max_movie_id = unrated_with_full_metadata.agg({\"itemId\": \"max\"}).collect()[0][0]\n",
    "num_rating_labels = unrated_with_full_metadata.select('label').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_size = 100\n",
    "wide_base_cols = movie_genres + user_avgs_genres\n",
    "wide_base_dims = [3 for i in movie_genres] + [11 for i in user_avgs_genres]\n",
    "# indicator_cols = indicator_base_fields + movie_genres\n",
    "# indicator_dims = [3 for i in indicator_cols]\n",
    "continuous_cols = continuous_base_fields + user_avgs_genres\n",
    "column_info = ColumnFeatureInfo(\n",
    "            wide_base_cols = wide_base_cols,\n",
    "            wide_base_dims = wide_base_dims,\n",
    "            # wide_cross_cols = [\"age-gender\"],\n",
    "            # wide_cross_dims = [bucket_size],\n",
    "            # indicator_cols = indicator_cols,\n",
    "            # indicator_dims = indicator_dims,\n",
    "            embed_cols = [\"userId\", \"itemId\"],\n",
    "            embed_in_dims = [max_user_id, max_movie_id],\n",
    "            embed_out_dims = [500, 500],\n",
    "            continuous_cols = continuous_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.58 ms, sys: 6.21 ms, total: 12.8 ms\n",
      "Wall time: 2.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wnd_pred_rdd = unrated_with_full_metadata.rdd.map(lambda row: to_user_item_feature(row, column_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.44 ms, sys: 2.11 ms, total: 6.55 ms\n",
      "Wall time: 52.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "WnDModel = WideAndDeep.load_model(path = data_path + 'WnD_Model_20m.bigdl', \n",
    "                                  weight_path = data_path + 'WnD_Model_20m_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wnd_user_recs = WnDModel.recommend_for_user(wnd_pred_rdd, num_recs)\n",
    "# user_recs = [user_rec.item_id for user_rec in wnd_user_recs.take(num_recs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "wnd_movie_recs = movies.filter(col('item_id').isin(user_recs)).select('title', 'year')\n",
    "print 'Wind&Deep Recommendations'\n",
    "print wnd_movie_recs.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Function Recommendation Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fav_movies = ['Iron Man', 'Tinker Tailor Soldier Spy', 'Shawshank Redemption', 'Lord of the Rings', 'Harry Potter',\n",
    "             'The Family Stone', 'Shaun of the Dead', 'Up', 'A View to a Kill']\n",
    "new_user_input(fav_movies = fav_movies, all_ratings = ratings, \n",
    "               movies = movies, spark_context = sc, sqlContext = sqlContext,\n",
    "               num_recs = 10, movies_gp = movies_gp, movies_df = movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fav_movies = ['Tinker Tailor Soldier Spy', 'Shawshank Redemption', 'Lord of the Rings']\n",
    "new_user_input(fav_movies = fav_movies, all_ratings = ratings, \n",
    "               movies = movies, spark_context = sc, sqlContext = sqlContext,\n",
    "               num_recs = 10, movies_gp = movies_gp, movies_df = movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting favorite movie IDs\n",
      "Adding ratings to full set\n",
      "Creating prediction set\n",
      "Formatting training and prediction dataframes for NCF\n",
      "Training NCF Model\n",
      "creating: createZooNeuralCF\n",
      "creating: createClassNLLCriterion\n",
      "creating: createMaxEpoch\n",
      "creating: createAdam\n",
      "creating: createDistriOptimizer\n",
      "Making Predictions\n",
      "\n",
      "NCF Recommendations\n",
      "                                               title  year\n",
      "0  Heimat - A Chronicle of Germany (Heimat - Eine...     0\n",
      "1      Zero Motivation (Efes beyahasei enosh) (2014)  2014\n",
      "2                               North & South (2004)  2004\n",
      "3                                 Connections (1978)     0\n",
      "4                              Shepard & Dark (2012)  2012\n",
      "5                   Shawshank Redemption, The (1994)  1994\n",
      "6                         My Future Boyfriend (2011)  2011\n",
      "7  Lord of the Rings: The Fellowship of the Ring,...  2001\n",
      "8                    TT3D: Closer to the Edge (2011)  2011\n",
      "9                            Band of Brothers (2001)  2001\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'indicator_base_fields' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-fe6542b3b634>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu\"fav_movies = ['Frozen', 'Tangled', 'Oceans Eleven', 'Toy Story', 'The Princess Bride',  \\n              'The Incredibles', 'Castle in the Sky', 'Monsters, Inc']\\nnew_user_input(fav_movies = fav_movies, all_ratings = ratings, \\n               movies = movies, spark_context = sc, sqlContext = sqlContext,\\n               num_recs = 10, movies_gp = movies_gp, movies_df = movies_df)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-257081532a3f>\u001b[0m in \u001b[0;36mnew_user_input\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# Create column_info for feature formatting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mbucket_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mwide_base_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindicator_base_fields\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmovie_genres\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0muser_avgs_genres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0mwide_base_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindicator_base_fields\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmovie_genres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muser_avgs_genres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mindicator_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindicator_base_fields\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmovie_genres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'indicator_base_fields' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fav_movies = ['Frozen', 'Tangled', 'Oceans Eleven', 'Toy Story', 'The Princess Bride',  \n",
    "              'The Incredibles', 'Castle in the Sky', 'Monsters, Inc']\n",
    "new_user_input(fav_movies = fav_movies, all_ratings = ratings, \n",
    "               movies = movies, spark_context = sc, sqlContext = sqlContext,\n",
    "               num_recs = 10, movies_gp = movies_gp, movies_df = movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fav_movies = ['The Sound of Music', 'Blackhawk Down', 'Pearl Harbor', 'Toy Story', 'The Princess Bride',  \n",
    "              'Foreign Student', 'Star Wars', 'The Shining', 'Rear Window', 'Groundhog Day', 'Ghostbusters', \n",
    "              'Robin Hood (1993)', 'Die Hard']\n",
    "new_user_input(fav_movies = fav_movies, all_ratings = ratings, \n",
    "               movies = movies, spark_context = sc, sqlContext = sqlContext,\n",
    "               num_recs = 10, movies_gp = movies_gp, movies_df = movies_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
